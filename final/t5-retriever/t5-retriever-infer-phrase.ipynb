{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8052995,"sourceType":"datasetVersion","datasetId":4749278},{"sourceId":8204914,"sourceType":"datasetVersion","datasetId":4861131},{"sourceId":8209953,"sourceType":"datasetVersion","datasetId":4863826},{"sourceId":8229841,"sourceType":"datasetVersion","datasetId":4880165}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install accelerate -U\n! pip install evaluate\n! pip install sentence-transformers\n! pip install SentencePiece\n! pip install bert_score\n! pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:03:47.005128Z","iopub.execute_input":"2024-04-25T18:03:47.005713Z","iopub.status.idle":"2024-04-25T18:05:04.584507Z","shell.execute_reply.started":"2024-04-25T18:03:47.005684Z","shell.execute_reply":"2024-04-25T18:05:04.583390Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting responses<0.19 (from evaluate)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nInstalling collected packages: responses, evaluate\nSuccessfully installed evaluate-0.4.1 responses-0.18.0\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.7.0\nRequirement already satisfied: SentencePiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.4)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.39.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.31.0)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.22.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"#  import T5\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nimport numpy as np\nimport os\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DebertaTokenizer,\n    DebertaForSequenceClassification,\n    AutoConfig,\n)\n\nimport logging\nimport evaluate\nfrom evaluate import load\nfrom datasets import load_dataset\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import T5Tokenizer,T5ForConditionalGeneration\n\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu\n\nimport matplotlib.pyplot as plt\n\nfrom  transformers  import  AutoTokenizer, AutoModelWithLMHead\n\nimport csv\nfrom torch.utils.data import DataLoader, Dataset\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\nimport numpy as np\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\ndevice=\"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'Using GPU: {torch.cuda.get_device_name()}')\nelse:\n    device = torch.device(\"cpu\")\n    print('Using CPU')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:04.586853Z","iopub.execute_input":"2024-04-25T18:05:04.587336Z","iopub.status.idle":"2024-04-25T18:05:22.760604Z","shell.execute_reply.started":"2024-04-25T18:05:04.587296Z","shell.execute_reply":"2024-04-25T18:05:22.759673Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-25 18:05:14.306779: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-25 18:05:14.306878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-25 18:05:14.441713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"PATH=\"/kaggle/input/webis-clickbait-22/\"\nOUTPATH=\"/kaggle/working/\"\n# PATH=\"D:\\\\ghd\\\\NLP-Project\\\\webis-clickbait-22\\\\\"\n# OUTPATH=\"D:\\\\ghd\\\\NLP-Project\\\\webis-clickbait-22\\\\output\\\\\"\nBATCH=8\nT5_model=\"/kaggle/input/t5-retriver/retriver_phrase/T5_overall\"#phrase\n# T5_model=\"/kaggle/input/t5-retriver/retriever_passage/T5_overall_passage\"\n# T5_model=\"/kaggle/input/nlpprojt5/T5_overall_multi\"\nsep='[SEP]'","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:22.761920Z","iopub.execute_input":"2024-04-25T18:05:22.762755Z","iopub.status.idle":"2024-04-25T18:05:22.767886Z","shell.execute_reply.started":"2024-04-25T18:05:22.762718Z","shell.execute_reply":"2024-04-25T18:05:22.766903Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tag=\"phrase\"","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:22.770569Z","iopub.execute_input":"2024-04-25T18:05:22.770942Z","iopub.status.idle":"2024-04-25T18:05:22.805976Z","shell.execute_reply.started":"2024-04-25T18:05:22.770908Z","shell.execute_reply":"2024-04-25T18:05:22.805133Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:22.809020Z","iopub.execute_input":"2024-04-25T18:05:22.809312Z","iopub.status.idle":"2024-04-25T18:05:23.095999Z","shell.execute_reply.started":"2024-04-25T18:05:22.809289Z","shell.execute_reply":"2024-04-25T18:05:23.095099Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_escape(l):\n    return l.replace(\"\\n\",\". \")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:23.097288Z","iopub.execute_input":"2024-04-25T18:05:23.098092Z","iopub.status.idle":"2024-04-25T18:05:23.107425Z","shell.execute_reply.started":"2024-04-25T18:05:23.098057Z","shell.execute_reply":"2024-04-25T18:05:23.106551Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_label(x):\n    return \" \".join(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:23.108727Z","iopub.execute_input":"2024-04-25T18:05:23.109252Z","iopub.status.idle":"2024-04-25T18:05:23.117972Z","shell.execute_reply.started":"2024-04-25T18:05:23.109226Z","shell.execute_reply":"2024-04-25T18:05:23.117183Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_json(\"/kaggle/input/retriever/final_data_train.json\")\nval_data=pd.read_json(\"/kaggle/input/retriever/final_data_validation.json\")\ntest_data=pd.read_json(\"/kaggle/input/retriever/final_data_test.json\")\n\ntrain_data = train_data.replace({None: ''})\nval_data = val_data.replace({None: ''})\ntest_data = test_data.replace({None: ''})\n\ntrain_real=pd.read_json(\"/kaggle/input/webis-clickbait-22/train.jsonl\",lines=True)\nval_real=pd.read_json(\"/kaggle/input/webis-clickbait-22/validation.jsonl\",lines=True)\ntest_real=pd.read_json(\"/kaggle/input/webis-clickbait-22/test.jsonl\",lines=True)\n\ntrain_labels=train_real['spoiler'].apply(get_label)\nval_labels=val_real['spoiler'].apply(get_label)\ntest_labels=test_real['spoiler'].apply(get_label)\n\ntrain_tags=train_real['tags'].apply(get_label)\nval_tags=val_real['tags'].apply(get_label)\ntest_tags=test_real['tags'].apply(get_label)\n# train_type=train_real['spoiler']\ntrain_data['exctractedParagraph']=train_data['exctractedParagraph'].apply(remove_escape)\nval_data['exctractedParagraph']=val_data['exctractedParagraph'].apply(remove_escape)\ntest_data['exctractedParagraph']=test_data['exctractedParagraph'].apply(remove_escape)\n\ntrain_data['context']=train_data['targetTitle'] + ' ' + train_data['targetDescription']+\" \"+train_data['exctractedParagraph']\nval_data['context']=val_data['targetTitle'] + ' ' + val_data['targetDescription']+\" \"+val_data['exctractedParagraph']\ntest_data['context']=test_data['targetTitle'] + ' ' + test_data['targetDescription']+\" \"+test_data['exctractedParagraph']\n\ntrain_data['question']=train_data['postText']+\"?\"\nval_data['question']=val_data['postText']+'?'\ntest_data['question']=test_data['postText']+\"?\"\n\ntrain_data['spoiler']=train_labels\nval_data['spoiler']=val_labels\ntest_data['spoiler']=test_labels\n\ntrain_data['tags']=train_tags\nval_data['tags']=val_tags\ntest_data['tags']=test_tags\n\ntrain_data=train_data[train_data['tags']==tag]\nval_data=val_data[val_data['tags']==tag]\ntest_data=test_data[test_data['tags']==tag]\n\ntrain_data=train_data[['question','context','spoiler']]\n\nval_data=val_data[['question','context','spoiler']]\ntest_data=test_data[['question','context','spoiler']]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:23.118933Z","iopub.execute_input":"2024-04-25T18:05:23.119208Z","iopub.status.idle":"2024-04-25T18:05:24.452829Z","shell.execute_reply.started":"2024-04-25T18:05:23.119186Z","shell.execute_reply":"2024-04-25T18:05:24.451758Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:24.454464Z","iopub.execute_input":"2024-04-25T18:05:24.454881Z","iopub.status.idle":"2024-04-25T18:05:24.470260Z","shell.execute_reply.started":"2024-04-25T18:05:24.454845Z","shell.execute_reply":"2024-04-25T18:05:24.469207Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                               question  \\\n1       NASA sets date for full recovery of ozone hole?   \n2     This is what makes employees happy -- and it's...   \n4     The perfect way to cook rice so that it's perf...   \n10    Analysis: This may be the most brutal number i...   \n11    #TeenMom2 star @PBandJenelley_1 reveals the se...   \n...                                                 ...   \n3178  He Dug A Huge Hole In His Backyard, And What H...   \n3179  Best Buy Has An Insane Xbox One Deal For A Lim...   \n3183  Student forced to carry papers to prove she ca...   \n3198  You need to see this Twitter account that pred...   \n3199        GOP congressman comes out for gay marriage?   \n\n                                                context  \\\n1     Hole In Ozone Layer Expected To Make Full Reco...   \n2     Intellectual Stimulation Trumps Money For Empl...   \n4     Revealed: The perfect way to cook rice so that...   \n10    This may be the most brutal number in the CBO ...   \n11    'Teen Mom 2' Star Jenelle Evans Reveals Sex Of...   \n...                                                 ...   \n3178  He Dug A Huge Hole In His Backyard, And What H...   \n3179  Best Buy Has An Insane Xbox One Deal For A Lim...   \n3183  Melona Clark, Hampton University Student, Carr...   \n3198  WTF, It Looks Like This Twitter Account \"Predi...   \n3199  Pennsylvania GOP Rep. Charlie Dent Comes Out F...   \n\n                                        spoiler  \n1                                          2070  \n2                      intellectual stimulation  \n4                              in a rice cooker  \n10                                  750 percent  \n11                                          boy  \n...                                         ...  \n3178                    own underground bunker!  \n3179                                    $50 off  \n3183  student at Hampton University in Virginia  \n3198                             @beyoncefan666  \n3199                  Rep. Charlie Dent (R-Pa.)  \n\n[1367 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>context</th>\n      <th>spoiler</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NASA sets date for full recovery of ozone hole?</td>\n      <td>Hole In Ozone Layer Expected To Make Full Reco...</td>\n      <td>2070</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This is what makes employees happy -- and it's...</td>\n      <td>Intellectual Stimulation Trumps Money For Empl...</td>\n      <td>intellectual stimulation</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The perfect way to cook rice so that it's perf...</td>\n      <td>Revealed: The perfect way to cook rice so that...</td>\n      <td>in a rice cooker</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Analysis: This may be the most brutal number i...</td>\n      <td>This may be the most brutal number in the CBO ...</td>\n      <td>750 percent</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>#TeenMom2 star @PBandJenelley_1 reveals the se...</td>\n      <td>'Teen Mom 2' Star Jenelle Evans Reveals Sex Of...</td>\n      <td>boy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3178</th>\n      <td>He Dug A Huge Hole In His Backyard, And What H...</td>\n      <td>He Dug A Huge Hole In His Backyard, And What H...</td>\n      <td>own underground bunker!</td>\n    </tr>\n    <tr>\n      <th>3179</th>\n      <td>Best Buy Has An Insane Xbox One Deal For A Lim...</td>\n      <td>Best Buy Has An Insane Xbox One Deal For A Lim...</td>\n      <td>$50 off</td>\n    </tr>\n    <tr>\n      <th>3183</th>\n      <td>Student forced to carry papers to prove she ca...</td>\n      <td>Melona Clark, Hampton University Student, Carr...</td>\n      <td>student at Hampton University in Virginia</td>\n    </tr>\n    <tr>\n      <th>3198</th>\n      <td>You need to see this Twitter account that pred...</td>\n      <td>WTF, It Looks Like This Twitter Account \"Predi...</td>\n      <td>@beyoncefan666</td>\n    </tr>\n    <tr>\n      <th>3199</th>\n      <td>GOP congressman comes out for gay marriage?</td>\n      <td>Pennsylvania GOP Rep. Charlie Dent Comes Out F...</td>\n      <td>Rep. Charlie Dent (R-Pa.)</td>\n    </tr>\n  </tbody>\n</table>\n<p>1367 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preproc(data):\n    q=[]\n    c=[]\n    a=[]\n    # print(len(data))\n    # print(len(data['question']))\n    # print(len(data['context']))\n    # print(len(data['spoiler']))\n    for i in range(len(data['question'])):\n        q.append(data['question'][i])\n        c.append(data['context'][i])\n        if c[i]==None:\n            c[i]=\"\"\n        a.append(str(data['spoiler'][i]))\n    model_inputs=tokenizer(q,c,text_target=a,return_tensors='pt',padding=True,truncation=True,max_length=512)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:24.473355Z","iopub.execute_input":"2024-04-25T18:05:24.473647Z","iopub.status.idle":"2024-04-25T18:05:24.480921Z","shell.execute_reply.started":"2024-04-25T18:05:24.473623Z","shell.execute_reply":"2024-04-25T18:05:24.479907Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(T5_model)\nmodel = AutoModelWithLMHead.from_pretrained(T5_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:24.482036Z","iopub.execute_input":"2024-04-25T18:05:24.482484Z","iopub.status.idle":"2024-04-25T18:05:32.464467Z","shell.execute_reply.started":"2024-04-25T18:05:24.482456Z","shell.execute_reply":"2024-04-25T18:05:32.463651Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1682: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyarrow as pa\nimport pyarrow.dataset as ds\nimport pandas as pd\nfrom datasets import Dataset\n\n### convert to Huggingface dataset\ntrain_dataset = Dataset(pa.Table.from_pandas(train_data)).remove_columns([\"__index_level_0__\"])\n\nval_dataset = Dataset(pa.Table.from_pandas(val_data)).remove_columns([\"__index_level_0__\"])\ntest_dataset = Dataset(pa.Table.from_pandas(test_data)).remove_columns([\"__index_level_0__\"])\ntrain_dataset,val_dataset,test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:32.465700Z","iopub.execute_input":"2024-04-25T18:05:32.465989Z","iopub.status.idle":"2024-04-25T18:05:32.575499Z","shell.execute_reply.started":"2024-04-25T18:05:32.465964Z","shell.execute_reply":"2024-04-25T18:05:32.574606Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['question', 'context', 'spoiler'],\n     num_rows: 1367\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler'],\n     num_rows: 335\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler'],\n     num_rows: 423\n }))"},"metadata":{}}]},{"cell_type":"code","source":"# train_data=Dataset.from_pandas(train_df)\ntokenized_train=train_dataset.map(preproc,batched=True,batch_size=BATCH)\ntokenized_val=val_dataset.map(preproc,batched=True,batch_size=BATCH)\ntokenized_test=test_dataset.map(preproc,batched=True,batch_size=BATCH)\ntokenized_train,tokenized_val,tokenized_test","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:32.576708Z","iopub.execute_input":"2024-04-25T18:05:32.578394Z","iopub.status.idle":"2024-04-25T18:05:35.258630Z","shell.execute_reply.started":"2024-04-25T18:05:32.578366Z","shell.execute_reply":"2024-04-25T18:05:35.257760Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1367 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b58da39efe4016adeee2d7babb9148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/335 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f23c064790145d8a87553f7614d2d62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/423 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634a098ea3b5450fbd018c593fcc0164"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['question', 'context', 'spoiler', 'input_ids', 'attention_mask', 'labels'],\n     num_rows: 1367\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler', 'input_ids', 'attention_mask', 'labels'],\n     num_rows: 335\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler', 'input_ids', 'attention_mask', 'labels'],\n     num_rows: 423\n }))"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=T5_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:35.259699Z","iopub.execute_input":"2024-04-25T18:05:35.259989Z","iopub.status.idle":"2024-04-25T18:05:35.264438Z","shell.execute_reply.started":"2024-04-25T18:05:35.259965Z","shell.execute_reply":"2024-04-25T18:05:35.263585Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(T5_model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:35.265745Z","iopub.execute_input":"2024-04-25T18:05:35.266002Z","iopub.status.idle":"2024-04-25T18:05:36.424723Z","shell.execute_reply.started":"2024-04-25T18:05:35.265980Z","shell.execute_reply":"2024-04-25T18:05:36.423799Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bertscore = load(\"bertscore\")\nmeteor = evaluate.load(\"meteor\")\nbleu = evaluate.load(\"bleu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:36.426098Z","iopub.execute_input":"2024-04-25T18:05:36.426747Z","iopub.status.idle":"2024-04-25T18:05:39.401144Z","shell.execute_reply.started":"2024-04-25T18:05:36.426702Z","shell.execute_reply":"2024-04-25T18:05:39.400310Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e6a591d80c4108b21e3cf4426d4500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f65e6e18c524fdf928e4c47c67f61de"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a1df313a9df44d194c07a5013950ddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3f9a8869653489fbd2d1bb55d2f7677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"247962757c204769a7c859b26d268f63"}},"metadata":{}}]},{"cell_type":"code","source":"\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    pred, ref = postprocess_text(decoded_preds, decoded_labels)\n\n    # cal bleu, meteor and bert\n    bleu_score = bleu.compute(predictions=pred, references=ref)\n#     bleu1 = corpus_bleu(ref,pred,weights=(1,0,0,0))\n#     bleu2 = corpus_bleu(ref,pred,weights=(0.5,0.5,0))\n#     bleu3 = corpus_bleu(ref,pred,weights=(0.33,0.33,0.33,0)\n#     bleu4 = corpus_bleu(ref,pred,weights=(0.25,0.25,0.25,0.25))\n\n    print(bleu_score)\n    meteor_score = meteor.compute(predictions=pred, references=ref)\n    bertscore_score = bertscore.compute(predictions=pred, references=ref, lang=\"en\")\n\n    #  dict\n    return {\"blue\":bleu_score[\"bleu\"],\n            \"precisions_1\":bleu_score[\"precisions\"][0],\n            \"precisions_2\":bleu_score[\"precisions\"][1],\n            \"precisions_3\":bleu_score[\"precisions\"][2],\n            \"precisions_4\":bleu_score[\"precisions\"][3],\n            \"bp\":bleu_score[\"brevity_penalty\"],\n            \"meteor\": meteor_score[\"meteor\"], \n            \"bertscore_f1\": np.average(bertscore_score[\"f1\"]), \n            \"bertscore_p\": np.average(bertscore_score[\"precision\"]), \n            \"bertscore_r\": np.average(bertscore_score[\"recall\"])}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:39.402479Z","iopub.execute_input":"2024-04-25T18:05:39.402755Z","iopub.status.idle":"2024-04-25T18:05:39.413017Z","shell.execute_reply.started":"2024-04-25T18:05:39.402731Z","shell.execute_reply":"2024-04-25T18:05:39.412170Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./T5\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-4,\n    per_device_train_batch_size=BATCH,\n    per_device_eval_batch_size=BATCH,\n    weight_decay=0.01,\n    save_total_limit=5,\n    num_train_epochs=12,\n    predict_with_generate=True,\n    save_strategy=\"epoch\",\n    fp16=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:39.414412Z","iopub.execute_input":"2024-04-25T18:05:39.414674Z","iopub.status.idle":"2024-04-25T18:05:40.070888Z","shell.execute_reply.started":"2024-04-25T18:05:39.414652Z","shell.execute_reply":"2024-04-25T18:05:40.070168Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_bleu(bp,precisions):\n    weights_1 = np.array([1,0,0,0])\n    weights_2 = np.array([0.5,0.5,0,0])\n    weights_3 = np.array([0.33,0.33,0.33,0])\n    weights_4 = np.array([0.25,0.25,0.25,0.25])\n    logp = np.log(precisions)\n        \n    res1 = bp*np.exp(np.dot(logp,weights_1))\n    res2 = bp*np.exp(np.dot(logp,weights_2))\n    res3 = bp*np.exp(np.dot(logp,weights_3))\n    res4 = bp*np.exp(np.dot(logp,weights_4))\n    \n    return [res1,res2,res3,res4]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:40.071908Z","iopub.execute_input":"2024-04-25T18:05:40.072188Z","iopub.status.idle":"2024-04-25T18:05:40.079965Z","shell.execute_reply.started":"2024-04-25T18:05:40.072152Z","shell.execute_reply":"2024-04-25T18:05:40.079150Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# results=trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:40.080812Z","iopub.execute_input":"2024-04-25T18:05:40.081057Z","iopub.status.idle":"2024-04-25T18:05:40.093234Z","shell.execute_reply.started":"2024-04-25T18:05:40.081036Z","shell.execute_reply":"2024-04-25T18:05:40.092425Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# calculate_bleu(results['eval_bp'],[results['eval_precisions_1'],results['eval_precisions_2'],results['eval_precisions_3'],results['eval_precisions_4']])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:40.094223Z","iopub.execute_input":"2024-04-25T18:05:40.094493Z","iopub.status.idle":"2024-04-25T18:05:40.104520Z","shell.execute_reply.started":"2024-04-25T18:05:40.094470Z","shell.execute_reply":"2024-04-25T18:05:40.103745Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import wandb\napi_key = \"9963cf6219e451d47251ea34645181ada1b2526b\"\nwandb.login(key=api_key)\nwandb.init()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:40.105562Z","iopub.execute_input":"2024-04-25T18:05:40.105826Z","iopub.status.idle":"2024-04-25T18:05:59.264220Z","shell.execute_reply.started":"2024-04-25T18:05:40.105803Z","shell.execute_reply":"2024-04-25T18:05:59.263252Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraghav21274\u001b[0m (\u001b[33mragha\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240425_180542-txjus9bg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ragha/uncategorized/runs/txjus9bg' target=\"_blank\">dashing-butterfly-130</a></strong> to <a href='https://wandb.ai/ragha/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ragha/uncategorized' target=\"_blank\">https://wandb.ai/ragha/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ragha/uncategorized/runs/txjus9bg' target=\"_blank\">https://wandb.ai/ragha/uncategorized/runs/txjus9bg</a>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ragha/uncategorized/runs/txjus9bg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79eca1a42800>"},"metadata":{}}]},{"cell_type":"code","source":"results = trainer.evaluate(eval_dataset=tokenized_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:05:59.265558Z","iopub.execute_input":"2024-04-25T18:05:59.266934Z","iopub.status.idle":"2024-04-25T18:06:42.235839Z","shell.execute_reply.started":"2024-04-25T18:05:59.266898Z","shell.execute_reply":"2024-04-25T18:06:42.234940Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [53/53 00:28]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'bleu': 0.4174322088957171, 'precisions': [0.5734388742304309, 0.484593837535014, 0.3805774278215223, 0.32701421800947866], 'brevity_penalty': 0.9679820099844161, 'length_ratio': 0.9684838160136287, 'translation_length': 1137, 'reference_length': 1174}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c633cd80714c4693b016d021de0d57af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a11e693a26e74bc7a1ff3b8fe33567a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6acc2fc271d346428fdeee6422e06d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b62600f84cd414b8f8b3ef902741d60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c78073c85388416babc734b51f39e225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e346ef747ae4a22ba3510b6866c1ec2"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 0.7351931929588318, 'eval_blue': 0.4174322088957171, 'eval_precisions_1': 0.5734388742304309, 'eval_precisions_2': 0.484593837535014, 'eval_precisions_3': 0.3805774278215223, 'eval_precisions_4': 0.32701421800947866, 'eval_bp': 0.9679820099844161, 'eval_meteor': 0.538161419365878, 'eval_bertscore_f1': 0.9328811870399096, 'eval_bertscore_p': 0.9342722582760714, 'eval_bertscore_r': 0.9322293750219594, 'eval_runtime': 42.9428, 'eval_samples_per_second': 9.85, 'eval_steps_per_second': 1.234}\n","output_type":"stream"}]},{"cell_type":"code","source":"calculate_bleu(results['eval_bp'],[results['eval_precisions_1'],results['eval_precisions_2'],results['eval_precisions_3'],results['eval_precisions_4']])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:06:42.237197Z","iopub.execute_input":"2024-04-25T18:06:42.237535Z","iopub.status.idle":"2024-04-25T18:06:42.247631Z","shell.execute_reply.started":"2024-04-25T18:06:42.237505Z","shell.execute_reply":"2024-04-25T18:06:42.246567Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[0.5550785140807734,\n 0.5102697170196723,\n 0.4611996199179566,\n 0.4174322088957171]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}