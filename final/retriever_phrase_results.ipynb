{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8052995,"sourceType":"datasetVersion","datasetId":4749278},{"sourceId":8204914,"sourceType":"datasetVersion","datasetId":4861131},{"sourceId":8209953,"sourceType":"datasetVersion","datasetId":4863826}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U torchdata\n# !pip install -U spacy\n# !python -m spacy download en_core_web_lg\n# !python -m spacy download de_core_news_lg\n# !pip install portalocker\n! pip install accelerate -U\n! pip install evaluate\n! pip install sentence-transformers\n! pip install SentencePiece\n! pip install bert_score\n! pip install --upgrade nltk","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:55:04.503449Z","iopub.execute_input":"2024-04-25T15:55:04.504314Z","iopub.status.idle":"2024-04-25T15:56:23.258725Z","shell.execute_reply.started":"2024-04-25T15:55:04.504278Z","shell.execute_reply":"2024-04-25T15:56:23.257725Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting responses<0.19 (from evaluate)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nInstalling collected packages: responses, evaluate\nSuccessfully installed evaluate-0.4.1 responses-0.18.0\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.22.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.7.0\nRequirement already satisfied: SentencePiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.1.4)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.39.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.31.0)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.22.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"#  import T5\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nimport numpy as np\nimport os\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DebertaTokenizer,\n    DebertaForSequenceClassification,\n    AutoConfig,\n)\n\nimport logging\nimport evaluate\nfrom evaluate import load\nfrom datasets import load_dataset\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import T5Tokenizer,T5ForConditionalGeneration\n\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu\n\nimport matplotlib.pyplot as plt\n\nfrom  transformers  import  AutoTokenizer, AutoModelWithLMHead\n\nimport csv\nfrom torch.utils.data import DataLoader, Dataset\nimport json\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\nimport numpy as np\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\ndevice=\"\"\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'Using GPU: {torch.cuda.get_device_name()}')\nelse:\n    device = torch.device(\"cpu\")\n    print('Using CPU')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:23.260903Z","iopub.execute_input":"2024-04-25T15:56:23.261266Z","iopub.status.idle":"2024-04-25T15:56:41.132475Z","shell.execute_reply.started":"2024-04-25T15:56:23.261232Z","shell.execute_reply":"2024-04-25T15:56:41.131511Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-25 15:56:32.640827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-25 15:56:32.640929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-25 15:56:32.780842: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"PATH=\"/kaggle/input/webis-clickbait-22/\"\nOUTPATH=\"/kaggle/working/\"\n# PATH=\"D:\\\\ghd\\\\NLP-Project\\\\webis-clickbait-22\\\\\"\n# OUTPATH=\"D:\\\\ghd\\\\NLP-Project\\\\webis-clickbait-22\\\\output\\\\\"\nBATCH=8\n# T5_model = \"t5-base\"\nT5_model=\"/kaggle/input/nlpprojt5/T5_overall\"\nsep='[SEP]'","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:41.133841Z","iopub.execute_input":"2024-04-25T15:56:41.134899Z","iopub.status.idle":"2024-04-25T15:56:41.139392Z","shell.execute_reply.started":"2024-04-25T15:56:41.134861Z","shell.execute_reply":"2024-04-25T15:56:41.138506Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n# from torch.utils.data import DataLoader\n# from torch.utils.data import DataLoader,Dataset\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# import torch\n# from sentence_transformers import SentenceTransformer,util\n# import json\n# from langchain.document_loaders import CSVLoader,TextLoader\n# from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n# from langchain.vectorstores import FAISS\n# import pandas as pd\n# from langchain.text_splitter import RecursiveCharacterTextSplitter","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:29.928213Z","iopub.execute_input":"2024-04-23T20:17:29.928467Z","iopub.status.idle":"2024-04-23T20:17:29.943411Z","shell.execute_reply.started":"2024-04-23T20:17:29.928445Z","shell.execute_reply":"2024-04-23T20:17:29.942572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from langchain.embeddings import HuggingFaceInstructEmbeddings\n# from InstructorEmbedding import INSTRUCTOR\n# from tqdm.notebook import tqdm\n# embedding = SentenceTransformerEmbeddings(model_name=\"hkunlp/instructor-xl\",model_kwargs={\"device\":\"cuda\"})","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:29.945832Z","iopub.execute_input":"2024-04-23T20:17:29.946117Z","iopub.status.idle":"2024-04-23T20:17:29.955016Z","shell.execute_reply.started":"2024-04-23T20:17:29.946086Z","shell.execute_reply":"2024-04-23T20:17:29.954223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def final_creator(file):\n#     final_dict={'id':[],'postText':[],'targetTitle':[],'targetDescription':[],\"exctractedParagraph\":[]}\n#     data=pd.read_json(path+file,lines=True)\n#     progress=tqdm(range(len(data)))\n#     for i in progress:\n#         q=data.iloc[i]\n#         l=q['postText'][0]\n#         a=q['targetParagraphs']\n#         target_title=q['targetTitle']\n#         a=\"\\n\".join(a)\n#         with open(\"a.txt\",\"w\") as f:\n#             f.write(a)\n#         loader = TextLoader(\"a.txt\")\n#         documents = loader.load()\n#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=100)\n#         texts = text_splitter.split_documents(documents)\n#         db_embeddings = FAISS.from_documents(texts,embedding)\n#         retriever = db_embeddings.as_retriever(search_kwargs={\"k\":2})\n#         top_triples = retriever.get_relevant_documents(l)\n#         context=\"\"\n#         for i in top_triples:\n#             context+=i.page_content\n#         final_dict['id'].append(q['uuid'])\n#         final_dict['postText'].append(l)\n#         final_dict['targetTitle'].append(target_title)\n#         final_dict['targetDescription'].append(q['targetDescription'])\n#         final_dict['exctractedParagraph'].append(context)\n#     final_data=pd.DataFrame(final_dict)\n#     final_data.to_json(\"retriver/final_data_\"+file.split(\".\")[0]+\".json\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:29.955885Z","iopub.execute_input":"2024-04-23T20:17:29.956133Z","iopub.status.idle":"2024-04-23T20:17:29.965539Z","shell.execute_reply.started":"2024-04-23T20:17:29.95611Z","shell.execute_reply":"2024-04-23T20:17:29.964807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_file=\"train.jsonl\"\n# val_file=\"validation.jsonl\"\n# test_file=\"test.jsonl\"\n# final_creator(train_file)\n# final_creator(val_file)\n# final_creator(test_file)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:29.966464Z","iopub.execute_input":"2024-04-23T20:17:29.96673Z","iopub.status.idle":"2024-04-23T20:17:29.979727Z","shell.execute_reply.started":"2024-04-23T20:17:29.966708Z","shell.execute_reply":"2024-04-23T20:17:29.978934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:41.141384Z","iopub.execute_input":"2024-04-25T15:56:41.141743Z","iopub.status.idle":"2024-04-25T15:56:41.462272Z","shell.execute_reply.started":"2024-04-25T15:56:41.141717Z","shell.execute_reply":"2024-04-25T15:56:41.461129Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_escape(l):\n    return l.replace(\"\\n\",\". \")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:41.466242Z","iopub.execute_input":"2024-04-25T15:56:41.466592Z","iopub.status.idle":"2024-04-25T15:56:41.474501Z","shell.execute_reply.started":"2024-04-25T15:56:41.466564Z","shell.execute_reply":"2024-04-25T15:56:41.473127Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_label(x):\n    return \" \".join(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:41.476576Z","iopub.execute_input":"2024-04-25T15:56:41.477352Z","iopub.status.idle":"2024-04-25T15:56:41.487388Z","shell.execute_reply.started":"2024-04-25T15:56:41.477320Z","shell.execute_reply":"2024-04-25T15:56:41.486368Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_json(\"/kaggle/input/retriever/final_data_train.json\")\nval_data=pd.read_json(\"/kaggle/input/retriever/final_data_validation.json\")\ntest_data=pd.read_json(\"/kaggle/input/retriever/final_data_test.json\")\n\ntrain_data = train_data.replace({None: ''})\nval_data = val_data.replace({None: ''})\ntest_data = test_data.replace({None: ''})\n\ntrain_real=pd.read_json(\"/kaggle/input/webis-clickbait-22/train.jsonl\",lines=True)\nval_real=pd.read_json(\"/kaggle/input/webis-clickbait-22/validation.jsonl\",lines=True)\ntest_real=pd.read_json(\"/kaggle/input/webis-clickbait-22/test.jsonl\",lines=True)\n\ntrain_labels=train_real['spoiler'].apply(get_label)\nval_labels=val_real['spoiler'].apply(get_label)\ntest_labels=test_real['spoiler'].apply(get_label)\n\ntrain_tags=train_real['tags'].apply(get_label)\nval_tags=val_real['tags'].apply(get_label)\ntest_tags=test_real['tags'].apply(get_label)\n# train_type=train_real['spoiler']\ntrain_data['exctractedParagraph']=train_data['exctractedParagraph'].apply(remove_escape)\nval_data['exctractedParagraph']=val_data['exctractedParagraph'].apply(remove_escape)\ntest_data['exctractedParagraph']=test_data['exctractedParagraph'].apply(remove_escape)\n\ntrain_data['context']=train_data['targetTitle'] + ' ' + train_data['targetDescription']+\" \"+train_data['exctractedParagraph']\nval_data['context']=val_data['targetTitle'] + ' ' + val_data['targetDescription']+\" \"+val_data['exctractedParagraph']\ntest_data['context']=test_data['targetTitle'] + ' ' + test_data['targetDescription']+\" \"+test_data['exctractedParagraph']\n\ntrain_data['question']=train_data['postText']+\"?\"\nval_data['question']=val_data['postText']+'?'\ntest_data['question']=test_data['postText']+\"?\"\n\ntrain_data['spoiler']=train_labels\nval_data['spoiler']=val_labels\ntest_data['spoiler']=test_labels\n\ntrain_data['tags']=train_tags\nval_data['tags']=val_tags\ntest_data['tags']=test_tags\n\ntrain_data=train_data[train_data['tags']=='phrase']\nval_data=val_data[val_data['tags']=='phrase']\ntest_data=test_data[test_data['tags']=='phrase']\n\ntrain_data=train_data[['question','context','spoiler']]\n\nval_data=val_data[['question','context','spoiler']]\ntest_data=test_data[['question','context','spoiler']]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:41.488841Z","iopub.execute_input":"2024-04-25T15:56:41.489348Z","iopub.status.idle":"2024-04-25T15:56:42.998055Z","shell.execute_reply.started":"2024-04-25T15:56:41.489318Z","shell.execute_reply":"2024-04-25T15:56:42.997234Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:31.136571Z","iopub.execute_input":"2024-04-23T20:17:31.136997Z","iopub.status.idle":"2024-04-23T20:17:31.155577Z","shell.execute_reply.started":"2024-04-23T20:17:31.136958Z","shell.execute_reply":"2024-04-23T20:17:31.154643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_data.to_csv(\"check.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:31.156781Z","iopub.execute_input":"2024-04-23T20:17:31.157132Z","iopub.status.idle":"2024-04-23T20:17:31.161558Z","shell.execute_reply.started":"2024-04-23T20:17:31.157098Z","shell.execute_reply":"2024-04-23T20:17:31.160553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t5\ntokenizer = AutoTokenizer.from_pretrained(T5_model)\nmodel = AutoModelWithLMHead.from_pretrained(T5_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:42.999452Z","iopub.execute_input":"2024-04-25T15:56:43.000328Z","iopub.status.idle":"2024-04-25T15:56:53.451683Z","shell.execute_reply.started":"2024-04-25T15:56:43.000288Z","shell.execute_reply":"2024-04-25T15:56:53.450891Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1682: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyarrow as pa\nimport pyarrow.dataset as ds\nimport pandas as pd\nfrom datasets import Dataset\n\n### convert to Huggingface dataset\ntrain_dataset = Dataset(pa.Table.from_pandas(train_data)).remove_columns([\"__index_level_0__\"])\n\nval_dataset = Dataset(pa.Table.from_pandas(val_data)).remove_columns([\"__index_level_0__\"])\ntest_dataset = Dataset(pa.Table.from_pandas(test_data)).remove_columns([\"__index_level_0__\"])\ntrain_dataset,val_dataset,test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:53.452932Z","iopub.execute_input":"2024-04-25T15:56:53.453268Z","iopub.status.idle":"2024-04-25T15:56:53.564585Z","shell.execute_reply.started":"2024-04-25T15:56:53.453240Z","shell.execute_reply":"2024-04-25T15:56:53.563632Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['question', 'context', 'spoiler'],\n     num_rows: 1367\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler'],\n     num_rows: 335\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler'],\n     num_rows: 423\n }))"},"metadata":{}}]},{"cell_type":"code","source":"def preproc(data):\n    q=[]\n    c=[]\n    a=[]\n    # print(len(data))\n    # print(len(data['question']))\n    # print(len(data['context']))\n    # print(len(data['spoiler']))\n    for i in range(len(data['question'])):\n        q.append(data['question'][i])\n        c.append(data['context'][i])\n        if c[i]==None:\n            c[i]=\"\"\n        a.append(str(data['spoiler'][i]))\n    model_inputs=tokenizer(q,c,text_target=a,return_tensors='pt',padding=True,truncation=True,max_length=512)\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:53.568473Z","iopub.execute_input":"2024-04-25T15:56:53.568737Z","iopub.status.idle":"2024-04-25T15:56:53.575107Z","shell.execute_reply.started":"2024-04-25T15:56:53.568715Z","shell.execute_reply":"2024-04-25T15:56:53.574050Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# train_data=Dataset.from_pandas(train_df)\ntokenized_train=train_dataset.map(preproc,batched=True,batch_size=BATCH)\ntokenized_val=val_dataset.map(preproc,batched=True,batch_size=BATCH)\ntokenized_test=test_dataset.map(preproc,batched=True,batch_size=BATCH)\ntokenized_train,tokenized_val,tokenized_test","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:53.576208Z","iopub.execute_input":"2024-04-25T15:56:53.576533Z","iopub.status.idle":"2024-04-25T15:56:56.166585Z","shell.execute_reply.started":"2024-04-25T15:56:53.576503Z","shell.execute_reply":"2024-04-25T15:56:56.165707Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1367 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38f9d65e4294c08a295b2bd45548ce1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/335 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0058c8fba3ed48249897be680397b692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/423 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3520ac041d564ff28d53598b94d28962"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(Dataset({\n     features: ['question', 'context', 'spoiler', 'input_ids', 'attention_mask', 'labels'],\n     num_rows: 1367\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler', 'input_ids', 'attention_mask', 'labels'],\n     num_rows: 335\n }),\n Dataset({\n     features: ['question', 'context', 'spoiler', 'input_ids', 'attention_mask', 'labels'],\n     num_rows: 423\n }))"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=T5_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:56.167705Z","iopub.execute_input":"2024-04-25T15:56:56.167984Z","iopub.status.idle":"2024-04-25T15:56:56.172419Z","shell.execute_reply.started":"2024-04-25T15:56:56.167960Z","shell.execute_reply":"2024-04-25T15:56:56.171585Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import wandb\napi_key = \"9963cf6219e451d47251ea34645181ada1b2526b\"\nwandb.login(key=api_key)\nwandb.init()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:01:34.748659Z","iopub.execute_input":"2024-04-25T16:01:34.749538Z","iopub.status.idle":"2024-04-25T16:01:52.429291Z","shell.execute_reply.started":"2024-04-25T16:01:34.749505Z","shell.execute_reply":"2024-04-25T16:01:52.428210Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraghav21274\u001b[0m (\u001b[33mragha\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240425_160134-gu8ovwfh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ragha/uncategorized/runs/gu8ovwfh' target=\"_blank\">lively-sun-117</a></strong> to <a href='https://wandb.ai/ragha/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ragha/uncategorized' target=\"_blank\">https://wandb.ai/ragha/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ragha/uncategorized/runs/gu8ovwfh' target=\"_blank\">https://wandb.ai/ragha/uncategorized/runs/gu8ovwfh</a>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ragha/uncategorized/runs/gu8ovwfh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d1148212e60>"},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(T5_model)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:56.173478Z","iopub.execute_input":"2024-04-25T15:56:56.173750Z","iopub.status.idle":"2024-04-25T15:56:57.338814Z","shell.execute_reply.started":"2024-04-25T15:56:56.173715Z","shell.execute_reply":"2024-04-25T15:56:57.337937Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bertscore = load(\"bertscore\")\nmeteor = evaluate.load(\"meteor\")\nbleu = evaluate.load(\"bleu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:57.339996Z","iopub.execute_input":"2024-04-25T15:56:57.340297Z","iopub.status.idle":"2024-04-25T15:56:59.996337Z","shell.execute_reply.started":"2024-04-25T15:56:57.340272Z","shell.execute_reply":"2024-04-25T15:56:59.995441Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd6b6184f9e462d90c1850691f0fa2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1a8566448984bbfa396ae6a72805827"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f10e6023e64b1b91f99057f5924563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ab58afef304ebca222eaf0befc4b45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d1bbfec193b462691c846d61078da8b"}},"metadata":{}}]},{"cell_type":"code","source":"\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    pred, ref = postprocess_text(decoded_preds, decoded_labels)\n\n    # cal bleu, meteor and bert\n    bleu_score = bleu.compute(predictions=pred, references=ref)\n#     bleu1 = corpus_bleu(ref,pred,weights=(1,0,0,0))\n#     bleu2 = corpus_bleu(ref,pred,weights=(0.5,0.5,0))\n#     bleu3 = corpus_bleu(ref,pred,weights=(0.33,0.33,0.33,0)\n#     bleu4 = corpus_bleu(ref,pred,weights=(0.25,0.25,0.25,0.25))\n\n    print(bleu_score)\n    meteor_score = meteor.compute(predictions=pred, references=ref)\n    bertscore_score = bertscore.compute(predictions=pred, references=ref, lang=\"en\")\n\n    #  dict\n    return {\"blue\":bleu_score[\"bleu\"],\n            \"precisions_1\":bleu_score[\"precisions\"][0],\n            \"precisions_2\":bleu_score[\"precisions\"][1],\n            \"precisions_3\":bleu_score[\"precisions\"][2],\n            \"precisions_4\":bleu_score[\"precisions\"][3],\n            \"bp\":bleu_score[\"brevity_penalty\"],\n            \"meteor\": meteor_score[\"meteor\"], \n            \"bertscore_f1\": np.average(bertscore_score[\"f1\"]), \n            \"bertscore_p\": np.average(bertscore_score[\"precision\"]), \n            \"bertscore_r\": np.average(bertscore_score[\"recall\"])}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:56:59.997682Z","iopub.execute_input":"2024-04-25T15:56:59.998367Z","iopub.status.idle":"2024-04-25T15:57:00.009548Z","shell.execute_reply.started":"2024-04-25T15:56:59.998331Z","shell.execute_reply":"2024-04-25T15:57:00.008650Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./T5\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-4,\n    per_device_train_batch_size=BATCH,\n    per_device_eval_batch_size=BATCH,\n    weight_decay=0.01,\n    save_total_limit=5,\n    num_train_epochs=12,\n    predict_with_generate=True,\n    save_strategy=\"epoch\",\n    fp16=True,\n#     report_to=\"wandb\",\n#     logging_dir='./lolololol'\n\n    # push_to_hub=True\n    # load_best_model_at_end=True,\n    # metric_for_best_model=\"bleu\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:57:00.010742Z","iopub.execute_input":"2024-04-25T15:57:00.011084Z","iopub.status.idle":"2024-04-25T15:57:00.688120Z","shell.execute_reply.started":"2024-04-25T15:57:00.011052Z","shell.execute_reply":"2024-04-25T15:57:00.687367Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_bleu(bp,precisions):\n    weights_1 = np.array([1,0,0,0])\n    weights_2 = np.array([0.5,0.5,0,0])\n    weights_3 = np.array([0.33,0.33,0.33,0])\n    weights_4 = np.array([0.25,0.25,0.25,0.25])\n    logp = np.log(precisions)\n        \n    res1 = bp*np.exp(np.dot(logp,weights_1))\n    res2 = bp*np.exp(np.dot(logp,weights_2))\n    res3 = bp*np.exp(np.dot(logp,weights_3))\n    res4 = bp*np.exp(np.dot(logp,weights_4))\n    \n    return [res1,res2,res3,res4]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T15:57:00.689328Z","iopub.execute_input":"2024-04-25T15:57:00.690006Z","iopub.status.idle":"2024-04-25T15:57:00.697171Z","shell.execute_reply.started":"2024-04-25T15:57:00.689971Z","shell.execute_reply":"2024-04-25T15:57:00.696233Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# results=trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:33.109726Z","iopub.status.idle":"2024-04-23T20:17:33.110177Z","shell.execute_reply.started":"2024-04-23T20:17:33.109946Z","shell.execute_reply":"2024-04-23T20:17:33.109965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate_bleu(results['eval_bp'],[results['eval_precisions_1'],results['eval_precisions_2'],results['eval_precisions_3'],results['eval_precisions_4']])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:33.111263Z","iopub.status.idle":"2024-04-23T20:17:33.11174Z","shell.execute_reply.started":"2024-04-23T20:17:33.111478Z","shell.execute_reply":"2024-04-23T20:17:33.111497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"T5_overall\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:33.114835Z","iopub.status.idle":"2024-04-23T20:17:33.11518Z","shell.execute_reply.started":"2024-04-23T20:17:33.115013Z","shell.execute_reply":"2024-04-23T20:17:33.115027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:33.116222Z","iopub.status.idle":"2024-04-23T20:17:33.11655Z","shell.execute_reply.started":"2024-04-23T20:17:33.116373Z","shell.execute_reply":"2024-04-23T20:17:33.116386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = trainer.evaluate(eval_dataset=tokenized_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:01:59.972530Z","iopub.execute_input":"2024-04-25T16:01:59.973453Z","iopub.status.idle":"2024-04-25T16:02:29.125040Z","shell.execute_reply.started":"2024-04-25T16:01:59.973410Z","shell.execute_reply":"2024-04-25T16:02:29.124089Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'bleu': 0.4174322088957171, 'precisions': [0.5734388742304309, 0.484593837535014, 0.3805774278215223, 0.32701421800947866], 'brevity_penalty': 0.9679820099844161, 'length_ratio': 0.9684838160136287, 'translation_length': 1137, 'reference_length': 1174}\n{'eval_loss': 0.7351931929588318, 'eval_blue': 0.4174322088957171, 'eval_precisions_1': 0.5734388742304309, 'eval_precisions_2': 0.484593837535014, 'eval_precisions_3': 0.3805774278215223, 'eval_precisions_4': 0.32701421800947866, 'eval_bp': 0.9679820099844161, 'eval_meteor': 0.538161419365878, 'eval_bertscore_f1': 0.932881191971736, 'eval_bertscore_p': 0.9342722616578952, 'eval_bertscore_r': 0.9322293803765137, 'eval_runtime': 29.138, 'eval_samples_per_second': 14.517, 'eval_steps_per_second': 1.819}\n","output_type":"stream"}]},{"cell_type":"code","source":"calculate_bleu(results['eval_bp'],[results['eval_precisions_1'],results['eval_precisions_2'],results['eval_precisions_3'],results['eval_precisions_4']])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T16:02:31.905959Z","iopub.execute_input":"2024-04-25T16:02:31.906741Z","iopub.status.idle":"2024-04-25T16:02:31.916329Z","shell.execute_reply.started":"2024-04-25T16:02:31.906705Z","shell.execute_reply":"2024-04-25T16:02:31.915162Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[0.5550785140807734,\n 0.5102697170196723,\n 0.4611996199179566,\n 0.4174322088957171]"},"metadata":{}}]},{"cell_type":"code","source":"# # idx=\n# df=test_data\n# for idx in range(len(test_data)):\n#     q=df.iloc[idx]['question']\n#     c=df.iloc[idx]['context']\n#     enc=tokenizer(q+\" \"+c,return_tensors='pt',padding=True,truncation=True,max_length=512)\n#     out=model.generate(input_ids=enc['input_ids'].to(device))\n#     out=out.cpu()\n#     print(idx)\n#     for i in range(len(out)):\n#         print(\"predicted_spoiler\",tokenizer.decode(out[i],skip_special_tokens=True))\n#         print(\"actual_spoiler\",df.iloc[idx]['spoiler'])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T20:17:33.121191Z","iopub.status.idle":"2024-04-23T20:17:33.121496Z","shell.execute_reply.started":"2024-04-23T20:17:33.121345Z","shell.execute_reply":"2024-04-23T20:17:33.121357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}